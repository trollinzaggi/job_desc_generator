{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JD Corpus Analysis Notebook\n",
    "\n",
    "This notebook runs Phase 1 analysis using your configuration from `config.py`.\n",
    "\n",
    "## Configuration is Centralized\n",
    "\n",
    "Edit `config.py` to change:\n",
    "- **Which text field to analyze** (e.g., `jd_expertise` instead of `jd_text`)\n",
    "- **Stratification dimensions** (e.g., `rank` instead of `org_unit`)\n",
    "- **Cluster metadata fields** (e.g., add `hiring_manager`)\n",
    "- **Output directory** (via `OUTPUT_CONFIG`)\n",
    "\n",
    "You don't need to change any code in this notebook - just edit config.py and re-run.\n",
    "\n",
    "## Prerequisites\n",
    "1. JSON files in `jd_data/`\n",
    "2. `config.py` configured with your field mappings\n",
    "3. Packages installed: `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PLOTLY SETUP - Run this first!\n",
    "# =============================================================================\n",
    "# This configures Plotly to open interactive plots in your browser\n",
    "# (VS Code notebooks don't render Plotly well natively)\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "# Open plots in browser for full interactivity\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "# Alternative options if browser doesn't work:\n",
    "# pio.renderers.default = \"png\"        # Static images (always works)\n",
    "# pio.renderers.default = \"notebook\"   # Try this for Jupyter\n",
    "# pio.renderers.default = \"vscode\"     # Try this for VS Code\n",
    "\n",
    "print(f\"Plotly configured to use: {pio.renderers.default}\")\n",
    "print(\"Plots will open in your default browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import analysis modules\n",
    "from src.data_loaders import JSONFileLoader, FieldMapping, SchemaDiscovery\n",
    "from src.analysis import (\n",
    "    DataCleaner,\n",
    "    QualityEvaluator,\n",
    "    StratifiedSampler,\n",
    "    StructureParser,\n",
    "    StructureAnalyzer,\n",
    "    EmbeddingGenerator,\n",
    "    ContentClusterer,\n",
    "    ClusterAnalyzer,\n",
    "    create_visualization_data,\n",
    ")\n",
    "\n",
    "# Import YOUR configuration (including output paths)\n",
    "from config import (\n",
    "    JSON_CONFIG, \n",
    "    JD_FIELD_MAPPING, \n",
    "    ANALYSIS_CONFIG,\n",
    "    OUTPUT_CONFIG,\n",
    "    EMBEDDING_CONFIG,\n",
    "    get_output_path,\n",
    "    get_phase_output_path,\n",
    "    get_embedding_generator,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    HAS_MATPLOTLIB = True\n",
    "except ImportError:\n",
    "    HAS_MATPLOTLIB = False\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    HAS_PLOTLY = True\n",
    "except ImportError:\n",
    "    HAS_PLOTLY = False\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Matplotlib: {HAS_MATPLOTLIB}\")\n",
    "print(f\"  Plotly: {HAS_PLOTLY}\")\n",
    "print(f\"  Output root: {get_output_path().absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Review Your Configuration\n",
    "\n",
    "Let's see what's configured in `config.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"YOUR CONFIGURATION (from config.py)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Field Mapping\n",
    "print(\"\\nFIELD MAPPING\")\n",
    "print(\"   Your JSON fields -> Standard names\")\n",
    "print(\"-\" * 40)\n",
    "mapping = JD_FIELD_MAPPING.to_dict()\n",
    "if mapping:\n",
    "    for field, path in mapping.items():\n",
    "        print(f\"   {field:20} <- {path}\")\n",
    "else:\n",
    "    print(\"    No mappings configured!\")\n",
    "    print(\"   Run schema discovery below, then edit config.py\")\n",
    "\n",
    "# Analysis Config\n",
    "print(\"\\nANALYSIS CONFIGURATION\")\n",
    "print(\"   What fields to use for each analysis\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Text to analyze:     {ANALYSIS_CONFIG.primary_text_field}\")\n",
    "print(f\"   Additional texts:    {ANALYSIS_CONFIG.additional_text_fields or 'None'}\")\n",
    "print(f\"   ID field:            {ANALYSIS_CONFIG.id_field}\")\n",
    "print(f\"   Stratify by:         {ANALYSIS_CONFIG.stratify_by_primary} x {ANALYSIS_CONFIG.stratify_by_secondary}\")\n",
    "print(f\"   Cluster metadata:    {ANALYSIS_CONFIG.cluster_metadata_fields}\")\n",
    "print(f\"   Purity field:        {ANALYSIS_CONFIG.cluster_purity_field}\")\n",
    "\n",
    "# Output Config\n",
    "print(\"\\nOUTPUT CONFIGURATION\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Root directory:      {OUTPUT_CONFIG['root_dir']}\")\n",
    "print(f\"   Absolute path:       {get_output_path().absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Schema Discovery (if needed)\n",
    "\n",
    "Run this to see your JSON structure and get field path suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data (no mapping)\n",
    "loader_raw = JSONFileLoader(\n",
    "    data_path=JSON_CONFIG[\"data_path\"],\n",
    "    content_key=JSON_CONFIG[\"content_key\"],\n",
    ")\n",
    "\n",
    "print(f\"Data source: {JSON_CONFIG['data_path']}\")\n",
    "print(f\"File stats: {loader_raw.get_file_stats()}\")\n",
    "print(f\"Total records: {loader_raw.count_records()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover schema\n",
    "schema = loader_raw.discover_schema(sample_size=100)\n",
    "print(schema.print_schema_tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested mappings (copy to config.py)\n",
    "print(\"SUGGESTED FIELD MAPPINGS\")\n",
    "print(\"   Copy these to config.py JD_FIELD_MAPPING:\")\n",
    "print(\"-\" * 40)\n",
    "for field, path in schema.suggest_field_mapping().items():\n",
    "    print(f'   {field}=\"{path}\",')\n",
    "\n",
    "print(\"\\nLIKELY TEXT FIELDS (by average length):\")\n",
    "for path, stats in schema.get_likely_text_fields()[:5]:\n",
    "    print(f\"   {path}: avg_length={stats['avg_length']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load and Clean Data\n",
    "\n",
    "Loads data using your field mapping, validates config, then cleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with your field mapping\n",
    "loader = JSONFileLoader(\n",
    "    data_path=JSON_CONFIG[\"data_path\"],\n",
    "    content_key=JSON_CONFIG[\"content_key\"],\n",
    "    field_mapping=JD_FIELD_MAPPING,\n",
    ")\n",
    "\n",
    "df_raw = loader.load_as_dataframe()\n",
    "print(f\"Loaded {len(df_raw)} records\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate your config against the actual data\n",
    "validation = ANALYSIS_CONFIG.validate(list(df_raw.columns))\n",
    "\n",
    "if validation[\"errors\"]:\n",
    "    print(\"CONFIGURATION ERRORS (fix in config.py):\")\n",
    "    for err in validation[\"errors\"]:\n",
    "        print(f\"   - {err}\")\n",
    "    raise ValueError(\"Fix configuration errors before proceeding\")\n",
    "else:\n",
    "    print(\"[OK] No configuration errors\")\n",
    "\n",
    "if validation[\"warnings\"]:\n",
    "    print(\"\\nWARNINGS (analysis will continue with reduced functionality):\")\n",
    "    for warn in validation[\"warnings\"]:\n",
    "        print(f\"   - {warn}\")\n",
    "else:\n",
    "    print(\"[OK] No configuration warnings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean using configured text field\n",
    "print(f\"Cleaning text field: {ANALYSIS_CONFIG.primary_text_field}\")\n",
    "\n",
    "cleaner = DataCleaner(\n",
    "    text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "    id_field=ANALYSIS_CONFIG.id_field,\n",
    "    min_text_length=50,\n",
    ")\n",
    "\n",
    "df, cleaning_stats = cleaner.clean(df_raw)\n",
    "print(cleaning_stats.print_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize field fill rates\n",
    "if HAS_MATPLOTLIB:\n",
    "    fill_rates = cleaning_stats.field_fill_rates\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(fill_rates) * 0.3)))\n",
    "    fields = list(fill_rates.keys())\n",
    "    rates = list(fill_rates.values())\n",
    "    colors = ['green' if r > 0.8 else 'orange' if r > 0.5 else 'red' for r in rates]\n",
    "    \n",
    "    ax.barh(fields, rates, color=colors)\n",
    "    ax.set_xlabel('Fill Rate')\n",
    "    ax.set_title('Field Fill Rates')\n",
    "    ax.axvline(x=0.8, color='green', linestyle='--', alpha=0.5, label='80%')\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Phase 1.1: Quality Baseline\n",
    "\n",
    "Creates stratified sample using `stratify_by_primary` x `stratify_by_secondary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available stratification fields\n",
    "primary_strat, secondary_strat = ANALYSIS_CONFIG.get_available_stratify_fields(list(df.columns))\n",
    "\n",
    "print(f\"Configured:  {ANALYSIS_CONFIG.stratify_by_primary} x {ANALYSIS_CONFIG.stratify_by_secondary}\")\n",
    "print(f\"Available:   {primary_strat or '(none)'} x {secondary_strat or '(none)'}\")\n",
    "\n",
    "if primary_strat and secondary_strat:\n",
    "    print(\"\\n[OK] Will use 2D stratified sampling\")\n",
    "elif primary_strat:\n",
    "    print(\"\\n[WARN] Will use 1D stratified sampling\")\n",
    "else:\n",
    "    print(\"\\n[WARN] Will use random sampling (no stratification fields)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View strata distribution (if available)\n",
    "if primary_strat and secondary_strat:\n",
    "    sampler = StratifiedSampler(\n",
    "        df,\n",
    "        org_unit_field=primary_strat,\n",
    "        level_field=secondary_strat,\n",
    "        id_field=ANALYSIS_CONFIG.id_field,\n",
    "    )\n",
    "    dist = sampler.get_strata_distribution()\n",
    "    print(f\"Total strata: {len(dist)}\")\n",
    "    display(dist.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stratification dimensions\n",
    "if HAS_MATPLOTLIB and primary_strat:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Primary dimension\n",
    "    counts = df[primary_strat].value_counts().head(15)\n",
    "    axes[0].barh(counts.index.astype(str), counts.values)\n",
    "    axes[0].set_title(f\"JDs by {primary_strat} (Top 15)\")\n",
    "    axes[0].set_xlabel(\"Count\")\n",
    "    \n",
    "    # Secondary dimension\n",
    "    if secondary_strat and secondary_strat != primary_strat:\n",
    "        counts2 = df[secondary_strat].value_counts().head(15)\n",
    "        axes[1].barh(counts2.index.astype(str), counts2.values)\n",
    "        axes[1].set_title(f\"JDs by {secondary_strat}\")\n",
    "        axes[1].set_xlabel(\"Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation sample\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "if primary_strat and secondary_strat:\n",
    "    evaluator = QualityEvaluator(\n",
    "        df=df,\n",
    "        text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "        id_field=ANALYSIS_CONFIG.id_field,\n",
    "        org_unit_field=primary_strat,\n",
    "        level_field=secondary_strat,\n",
    "    )\n",
    "    sample = evaluator.create_evaluation_sample(n=SAMPLE_SIZE)\n",
    "elif primary_strat:\n",
    "    evaluator = QualityEvaluator(\n",
    "        df=df,\n",
    "        text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "        id_field=ANALYSIS_CONFIG.id_field,\n",
    "        org_unit_field=primary_strat,\n",
    "        level_field=primary_strat,\n",
    "    )\n",
    "    sample = evaluator.create_evaluation_sample(n=SAMPLE_SIZE)\n",
    "else:\n",
    "    sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=42)\n",
    "    evaluator = QualityEvaluator(\n",
    "        df=df,\n",
    "        text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "        id_field=ANALYSIS_CONFIG.id_field,\n",
    "        org_unit_field=ANALYSIS_CONFIG.id_field,\n",
    "        level_field=ANALYSIS_CONFIG.id_field,\n",
    "    )\n",
    "    evaluator.sample_df = sample\n",
    "\n",
    "print(f\"Created sample of {len(sample)} JDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for human evaluation - using config output path\n",
    "OUTPUT_DIR = get_phase_output_path(\"phase_1_1_quality\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Include configured export fields\n",
    "export_fields = [ANALYSIS_CONFIG.id_field, ANALYSIS_CONFIG.primary_text_field]\n",
    "export_fields.extend([f for f in ANALYSIS_CONFIG.quality_export_fields if f in df.columns])\n",
    "print(f\"Including fields: {export_fields}\")\n",
    "\n",
    "evaluator.export_for_evaluation(\n",
    "    str(OUTPUT_DIR / \"jd_quality_evaluation.json\"),\n",
    "    include_fields=export_fields,\n",
    ")\n",
    "evaluator.export_for_evaluation_csv(str(OUTPUT_DIR / \"jd_quality_evaluation.csv\"))\n",
    "\n",
    "print(f\"\\n[OK] Exported to: {OUTPUT_DIR}/\")\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"   1. Open jd_quality_evaluation.csv\")\n",
    "print(\"   2. Score each eval_* column 1-5\")\n",
    "print(\"   3. Mark gold standards as TRUE\")\n",
    "print(\"   4. Run the import cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN AFTER FILLING IN CSV ===\n",
    "EVAL_FILE = OUTPUT_DIR / \"jd_quality_evaluation.csv\"\n",
    "\n",
    "if EVAL_FILE.exists():\n",
    "    count = evaluator.import_evaluations(str(EVAL_FILE))\n",
    "    if count > 0:\n",
    "        evaluator.print_quality_report()\n",
    "        \n",
    "        # Save results\n",
    "        results = evaluator.analyze_quality()\n",
    "        with open(OUTPUT_DIR / \"quality_analysis_results.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        print(f\"\\nResults saved to {OUTPUT_DIR}/quality_analysis_results.json\")\n",
    "    else:\n",
    "        print(\"No evaluations found. Fill in the eval_* columns first.\")\n",
    "else:\n",
    "    print(f\"File not found: {EVAL_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Phase 1.2: Structural Consistency\n",
    "\n",
    "Parses `primary_text_field` to extract sections (and `additional_text_fields` if configured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse primary text field\n",
    "print(f\"Parsing: {ANALYSIS_CONFIG.primary_text_field}\")\n",
    "\n",
    "structure_analyzer = StructureAnalyzer()\n",
    "parsed_jds = structure_analyzer.parse_corpus(\n",
    "    df,\n",
    "    text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "    id_field=ANALYSIS_CONFIG.id_field,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print structure report\n",
    "structure_analyzer.print_structure_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize section coverage\n",
    "if HAS_MATPLOTLIB:\n",
    "    coverage = structure_analyzer.get_section_coverage()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(coverage) * 0.4)))\n",
    "    sections = list(coverage.keys())\n",
    "    rates = list(coverage.values())\n",
    "    colors = ['green' if r > 0.7 else 'orange' if r > 0.4 else 'red' for r in rates]\n",
    "    \n",
    "    ax.barh(sections, rates, color=colors)\n",
    "    ax.set_xlabel('Coverage')\n",
    "    ax.set_title(f'Section Coverage in {ANALYSIS_CONFIG.primary_text_field}')\n",
    "    ax.axvline(x=0.7, color='green', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View example parsed structure\n",
    "if parsed_jds:\n",
    "    example = parsed_jds[0]\n",
    "    print(f\"Example JD: {example.jd_id}\")\n",
    "    print(f\"Sections found: {len(example.sections)}\")\n",
    "    print(f\"Section types: {example.section_names()}\")\n",
    "    \n",
    "    print(\"\\nSection previews:\")\n",
    "    for s in example.sections[:3]:\n",
    "        print(f\"  [{s['section_type']}] {s['header'][:40]}...\")\n",
    "        print(f\"     {s['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze additional text fields (if configured)\n",
    "additional_results = {}\n",
    "\n",
    "for text_field in ANALYSIS_CONFIG.additional_text_fields:\n",
    "    if text_field in df.columns:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ADDITIONAL: {text_field}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        add_analyzer = StructureAnalyzer()\n",
    "        add_analyzer.parse_corpus(\n",
    "            df,\n",
    "            text_field=text_field,\n",
    "            id_field=ANALYSIS_CONFIG.id_field,\n",
    "            show_progress=False,\n",
    "        )\n",
    "        \n",
    "        consistency = add_analyzer.measure_consistency()\n",
    "        print(f\"Sections/JD: {consistency['num_sections_stats']['mean']:.1f}\")\n",
    "        print(f\"Top sections: {list(consistency['section_coverage'].keys())[:5]}\")\n",
    "        \n",
    "        additional_results[text_field] = add_analyzer\n",
    "    else:\n",
    "        print(f\"[WARN] {text_field} not in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save structure results - using config output path\n",
    "OUTPUT_DIR_STRUCT = get_phase_output_path(\"phase_1_2_structure\")\n",
    "OUTPUT_DIR_STRUCT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "structure_analyzer.export_parsed_jds(str(OUTPUT_DIR_STRUCT / \"parsed_jd_structures.json\"))\n",
    "\n",
    "consistency = structure_analyzer.measure_consistency()\n",
    "with open(OUTPUT_DIR_STRUCT / \"structure_consistency.json\", \"w\") as f:\n",
    "    json.dump(consistency, f, indent=2, default=str)\n",
    "\n",
    "# Save additional text field results\n",
    "for text_field, analyzer in additional_results.items():\n",
    "    add_consistency = analyzer.measure_consistency()\n",
    "    with open(OUTPUT_DIR_STRUCT / f\"structure_consistency_{text_field}.json\", \"w\") as f:\n",
    "        json.dump(add_consistency, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR_STRUCT}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Phase 1.3: Content Clustering\n",
    "\n",
    "Embeds `primary_text_field`, clusters, and analyzes against `cluster_metadata_fields`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using config\n",
    "print(f\"Embedding: {ANALYSIS_CONFIG.primary_text_field}\")\n",
    "print(f\"Azure OpenAI deployment: {EMBEDDING_CONFIG['deployment_name']}\")\n",
    "\n",
    "embedder = get_embedding_generator()\n",
    "embeddings, ids = embedder.embed_dataframe(\n",
    "    df,\n",
    "    text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "    id_field=ANALYSIS_CONFIG.id_field,\n",
    ")\n",
    "\n",
    "print(f\"\\nShape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k\n",
    "clusterer = ContentClusterer(embeddings, ids)\n",
    "\n",
    "K_VALUES = [5, 10, 15, 20, 30, 50]\n",
    "k_scores = {}\n",
    "\n",
    "print(\"Testing k values...\")\n",
    "for k in K_VALUES:\n",
    "    if k < len(df):  # Can't have more clusters than samples\n",
    "        result = clusterer.kmeans(n_clusters=k)\n",
    "        k_scores[k] = result.silhouette_score\n",
    "        print(f\"  k={k:3d}: silhouette={result.silhouette_score:.4f}\")\n",
    "\n",
    "best_k = max(k_scores, key=k_scores.get)\n",
    "print(f\"\\nBest k={best_k} (silhouette={k_scores[best_k]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k selection\n",
    "if HAS_MATPLOTLIB:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(list(k_scores.keys()), list(k_scores.values()), 'bo-', linewidth=2, markersize=8)\n",
    "    ax.axvline(x=best_k, color='red', linestyle='--', label=f'Best k={best_k}')\n",
    "    ax.set_xlabel('Number of Clusters (k)')\n",
    "    ax.set_ylabel('Silhouette Score')\n",
    "    ax.set_title('Cluster Quality by k')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters with configured metadata fields\n",
    "SELECTED_K = best_k  # Or override\n",
    "\n",
    "best_result = clusterer.results[f\"kmeans_{SELECTED_K}\"]\n",
    "cluster_df = clusterer.get_cluster_assignments(f\"kmeans_{SELECTED_K}\")\n",
    "\n",
    "# Get available metadata fields\n",
    "metadata_fields = ANALYSIS_CONFIG.get_available_cluster_fields(list(df.columns))\n",
    "print(f\"Analyzing clusters against: {metadata_fields}\")\n",
    "\n",
    "cluster_analyzer = ClusterAnalyzer(\n",
    "    df, cluster_df,\n",
    "    id_field=ANALYSIS_CONFIG.id_field,\n",
    ")\n",
    "cluster_analyzer.print_cluster_report(metadata_fields=metadata_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster purity\n",
    "purity_field = ANALYSIS_CONFIG.cluster_purity_field\n",
    "purity = None\n",
    "\n",
    "if purity_field in df.columns:\n",
    "    purity = cluster_analyzer.compute_cluster_purity(purity_field)\n",
    "    print(f\"Cluster purity (vs {purity_field}): {purity['overall_purity']:.3f}\")\n",
    "    \n",
    "    print(\"\\nPurity by cluster:\")\n",
    "    for cid, info in sorted(purity[\"cluster_purities\"].items()):\n",
    "        print(f\"  Cluster {cid}: {info['purity']:.2f} (dominant: {info['dominant_label']})\")\n",
    "else:\n",
    "    print(f\"[WARN] Purity field '{purity_field}' not in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction for visualization\n",
    "viz_df = None\n",
    "\n",
    "try:\n",
    "    print(\"Reducing dimensions with UMAP...\")\n",
    "    reduced = clusterer.reduce_dimensions(method=\"umap\", n_components=2)\n",
    "    \n",
    "    viz_df = create_visualization_data(\n",
    "        reduced,\n",
    "        best_result.labels,\n",
    "        ids,\n",
    "        df,\n",
    "        id_field=ANALYSIS_CONFIG.id_field,\n",
    "    )\n",
    "    print(f\"[OK] Visualization data ready: {len(viz_df)} points\")\n",
    "    print(f\"Columns: {list(viz_df.columns)}\")\n",
    "    print(f\"x range: {viz_df['x'].min():.2f} to {viz_df['x'].max():.2f}\")\n",
    "    print(f\"y range: {viz_df['y'].min():.2f} to {viz_df['y'].max():.2f}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(viz_df.head())\n",
    "except ImportError:\n",
    "    print(\"[ERROR] UMAP not installed. Install with: pip install umap-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot - colored by cluster\n",
    "if HAS_PLOTLY and viz_df is not None and len(viz_df) > 0:\n",
    "    # Convert cluster to string for better color handling\n",
    "    viz_df['cluster_str'] = viz_df['cluster'].astype(str)\n",
    "    \n",
    "    hover_cols = [ANALYSIS_CONFIG.id_field, \"cluster\"]\n",
    "    hover_cols.extend([f for f in metadata_fields[:3] if f in viz_df.columns])\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        viz_df,\n",
    "        x=\"x\", y=\"y\",\n",
    "        color=\"cluster_str\",\n",
    "        hover_data=hover_cols,\n",
    "        title=f\"Content Clusters (k={SELECTED_K}, text={ANALYSIS_CONFIG.primary_text_field})\",\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "    fig.update_layout(width=900, height=700)\n",
    "    fig.show()\n",
    "elif viz_df is not None:\n",
    "    print(f\"[WARN] viz_df has {len(viz_df)} rows - nothing to plot\")\n",
    "else:\n",
    "    print(\"[WARN] No visualization data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot - colored by metadata field\n",
    "COLOR_BY = ANALYSIS_CONFIG.cluster_purity_field  # Change to any available field\n",
    "\n",
    "if HAS_PLOTLY and viz_df is not None and len(viz_df) > 0 and COLOR_BY in viz_df.columns:\n",
    "    fig = px.scatter(\n",
    "        viz_df,\n",
    "        x=\"x\", y=\"y\",\n",
    "        color=COLOR_BY,\n",
    "        hover_data=[ANALYSIS_CONFIG.id_field, \"cluster\"],\n",
    "        title=f\"Embeddings colored by {COLOR_BY}\",\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "    fig.update_layout(width=900, height=700)\n",
    "    fig.show()\n",
    "elif viz_df is not None:\n",
    "    print(f\"[WARN] {COLOR_BY} not available for coloring. Available columns: {list(viz_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster archetypes\n",
    "title_field = \"title\" if \"title\" in df.columns else ANALYSIS_CONFIG.id_field\n",
    "\n",
    "archetypes = cluster_analyzer.find_cluster_archetypes(\n",
    "    text_field=ANALYSIS_CONFIG.primary_text_field,\n",
    "    title_field=title_field,\n",
    "    n_examples=2,\n",
    ")\n",
    "\n",
    "print(\"CLUSTER ARCHETYPES\")\n",
    "print(\"=\" * 60)\n",
    "for cluster_id, archetype in list(archetypes.items())[:5]:\n",
    "    print(f\"\\nCluster {cluster_id} ({archetype['size']} JDs)\")\n",
    "    print(f\"  Common titles: {list(archetype['common_titles'].keys())[:3]}\")\n",
    "    if archetype['examples']:\n",
    "        print(f\"  Example: {archetype['examples'][0]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster additional text fields (if configured)\n",
    "additional_cluster_results = {}\n",
    "\n",
    "for text_field in ANALYSIS_CONFIG.additional_text_fields:\n",
    "    if text_field in df.columns:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CLUSTERING: {text_field}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        add_embeddings, add_ids = embedder.embed_dataframe(\n",
    "            df,\n",
    "            text_field=text_field,\n",
    "            id_field=ANALYSIS_CONFIG.id_field,\n",
    "        )\n",
    "        \n",
    "        add_clusterer = ContentClusterer(add_embeddings, add_ids)\n",
    "        add_result = add_clusterer.kmeans(n_clusters=SELECTED_K)\n",
    "        \n",
    "        print(f\"Silhouette: {add_result.silhouette_score:.4f}\")\n",
    "        print(f\"Cluster sizes: {add_result.cluster_sizes}\")\n",
    "        \n",
    "        additional_cluster_results[text_field] = {\n",
    "            \"clusterer\": add_clusterer,\n",
    "            \"result\": add_result,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all Phase 1.3 results - using config output path\n",
    "OUTPUT_DIR_CLUSTER = get_phase_output_path(\"phase_1_3_clustering\")\n",
    "OUTPUT_DIR_CLUSTER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save embeddings\n",
    "np.save(OUTPUT_DIR_CLUSTER / \"embeddings.npy\", embeddings)\n",
    "with open(OUTPUT_DIR_CLUSTER / \"embedding_ids.json\", \"w\") as f:\n",
    "    json.dump(ids, f)\n",
    "\n",
    "# Save cluster results\n",
    "cluster_df.to_csv(OUTPUT_DIR_CLUSTER / \"cluster_assignments.csv\", index=False)\n",
    "\n",
    "with open(OUTPUT_DIR_CLUSTER / \"k_scores.json\", \"w\") as f:\n",
    "    json.dump(k_scores, f, indent=2)\n",
    "\n",
    "composition = cluster_analyzer.analyze_cluster_composition(metadata_fields)\n",
    "with open(OUTPUT_DIR_CLUSTER / \"cluster_composition.json\", \"w\") as f:\n",
    "    json.dump(composition, f, indent=2, default=str)\n",
    "\n",
    "with open(OUTPUT_DIR_CLUSTER / \"cluster_archetypes.json\", \"w\") as f:\n",
    "    json.dump(archetypes, f, indent=2, default=str)\n",
    "\n",
    "if viz_df is not None:\n",
    "    viz_df.to_csv(OUTPUT_DIR_CLUSTER / \"visualization_data.csv\", index=False)\n",
    "\n",
    "# Save additional text field results\n",
    "for text_field, data in additional_cluster_results.items():\n",
    "    add_cluster_df = data[\"clusterer\"].get_cluster_assignments(f\"kmeans_{SELECTED_K}\")\n",
    "    add_cluster_df.to_csv(\n",
    "        OUTPUT_DIR_CLUSTER / f\"cluster_assignments_{text_field}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR_CLUSTER}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Load Previous Results (for visualization only)\n",
    "\n",
    "Use this section if you already ran `run_analysis.py` and just want to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load visualization data from previous run\n",
    "VIZ_FILE = get_phase_output_path(\"phase_1_3_clustering\") / \"visualization_data.csv\"\n",
    "\n",
    "print(f\"Looking for: {VIZ_FILE}\")\n",
    "print(f\"Absolute path: {VIZ_FILE.absolute()}\")\n",
    "print(f\"Exists: {VIZ_FILE.exists()}\")\n",
    "\n",
    "if VIZ_FILE.exists():\n",
    "    loaded_viz_df = pd.read_csv(VIZ_FILE)\n",
    "    print(f\"\\nLoaded {len(loaded_viz_df)} points\")\n",
    "    print(f\"Columns: {list(loaded_viz_df.columns)}\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(loaded_viz_df.dtypes)\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(loaded_viz_df.head())\n",
    "else:\n",
    "    print(f\"\\n[ERROR] File not found: {VIZ_FILE}\")\n",
    "    print(f\"\\nRun 'python run_analysis.py --phase 1.3' first, or run the cells above.\")\n",
    "    \n",
    "    # List what files exist in the output directory\n",
    "    cluster_dir = get_phase_output_path(\"phase_1_3_clustering\")\n",
    "    if cluster_dir.exists():\n",
    "        print(f\"\\nFiles in {cluster_dir}:\")\n",
    "        for f in cluster_dir.iterdir():\n",
    "            print(f\"  - {f.name}\")\n",
    "    else:\n",
    "        print(f\"\\nOutput directory does not exist: {cluster_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loaded visualization data\n",
    "if 'loaded_viz_df' in dir() and loaded_viz_df is not None and len(loaded_viz_df) > 0:\n",
    "    if HAS_PLOTLY:\n",
    "        # Check for required columns\n",
    "        if 'x' not in loaded_viz_df.columns or 'y' not in loaded_viz_df.columns:\n",
    "            print(f\"[ERROR] Missing x/y columns. Available: {list(loaded_viz_df.columns)}\")\n",
    "        else:\n",
    "            print(f\"Plotting {len(loaded_viz_df)} points...\")\n",
    "            print(f\"x range: {loaded_viz_df['x'].min():.2f} to {loaded_viz_df['x'].max():.2f}\")\n",
    "            print(f\"y range: {loaded_viz_df['y'].min():.2f} to {loaded_viz_df['y'].max():.2f}\")\n",
    "            \n",
    "            # Convert cluster to string for categorical coloring\n",
    "            if 'cluster' in loaded_viz_df.columns:\n",
    "                loaded_viz_df['cluster_str'] = loaded_viz_df['cluster'].astype(str)\n",
    "                color_col = 'cluster_str'\n",
    "            else:\n",
    "                color_col = None\n",
    "            \n",
    "            fig = px.scatter(\n",
    "                loaded_viz_df,\n",
    "                x=\"x\", y=\"y\",\n",
    "                color=color_col,\n",
    "                title=\"Content Clusters (from saved data)\",\n",
    "            )\n",
    "            fig.update_traces(marker=dict(size=8, opacity=0.7))\n",
    "            fig.update_layout(width=900, height=700)\n",
    "            fig.show()\n",
    "    else:\n",
    "        print(\"[WARN] Plotly not installed. Install with: pip install plotly\")\n",
    "else:\n",
    "    print(\"[WARN] No data loaded. Run the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1 ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDATA\")\n",
    "print(f\"   Total JDs: {len(df)}\")\n",
    "print(f\"   Text analyzed: {ANALYSIS_CONFIG.primary_text_field}\")\n",
    "if ANALYSIS_CONFIG.additional_text_fields:\n",
    "    print(f\"   Additional texts: {ANALYSIS_CONFIG.additional_text_fields}\")\n",
    "\n",
    "print(f\"\\nPHASE 1.1: QUALITY BASELINE\")\n",
    "print(f\"   Stratification: {primary_strat or 'random'} x {secondary_strat or 'N/A'}\")\n",
    "print(f\"   Sample size: {len(sample)}\")\n",
    "if evaluator.evaluations:\n",
    "    results = evaluator.analyze_quality()\n",
    "    print(f\"   Evaluated: {results['total_evaluated']}\")\n",
    "    print(f\"   Avg score: {results['overall_stats']['mean_average_score']:.2f}\")\n",
    "else:\n",
    "    print(f\"   Status: Awaiting evaluation\")\n",
    "\n",
    "print(f\"\\nPHASE 1.2: STRUCTURAL CONSISTENCY\")\n",
    "consistency = structure_analyzer.measure_consistency()\n",
    "print(f\"   Unique structures: {consistency['unique_structures']}\")\n",
    "print(f\"   Top pattern coverage: {consistency['top_structure_coverage']:.1%}\")\n",
    "print(f\"   Avg sections/JD: {consistency['num_sections_stats']['mean']:.1f}\")\n",
    "\n",
    "print(f\"\\nPHASE 1.3: CONTENT CLUSTERING\")\n",
    "print(f\"   Best k: {best_k} (silhouette={k_scores[best_k]:.4f})\")\n",
    "print(f\"   Metadata analyzed: {metadata_fields}\")\n",
    "if purity is not None:\n",
    "    print(f\"   Purity ({purity_field}): {purity['overall_purity']:.3f}\")\n",
    "\n",
    "print(f\"\\nOUTPUTS\")\n",
    "print(f\"   {get_phase_output_path('phase_1_1_quality')}/\")\n",
    "print(f\"   {get_phase_output_path('phase_1_2_structure')}/\")\n",
    "print(f\"   {get_phase_output_path('phase_1_3_clustering')}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration Reference\n",
    "\n",
    "To change analysis behavior, edit `config.py`:\n",
    "\n",
    "```python\n",
    "# Change output directory\n",
    "OUTPUT_CONFIG = {\n",
    "    \"root_dir\": \"/path/to/your/output\",\n",
    "    ...\n",
    "}\n",
    "\n",
    "# Analyze different text field\n",
    "ANALYSIS_CONFIG = AnalysisConfig(\n",
    "    primary_text_field=\"jd_expertise\",  # Instead of jd_text\n",
    ")\n",
    "\n",
    "# Stratify by different dimensions\n",
    "ANALYSIS_CONFIG = AnalysisConfig(\n",
    "    stratify_by_primary=\"rank\",      # Instead of org_unit\n",
    "    stratify_by_secondary=\"title\",   # Instead of level\n",
    ")\n",
    "\n",
    "# Analyze clusters against different fields\n",
    "ANALYSIS_CONFIG = AnalysisConfig(\n",
    "    cluster_metadata_fields=[\"rank\", \"title\", \"hiring_manager\"],\n",
    "    cluster_purity_field=\"rank\",\n",
    ")\n",
    "\n",
    "# Analyze multiple text fields\n",
    "ANALYSIS_CONFIG = AnalysisConfig(\n",
    "    primary_text_field=\"jd_text\",\n",
    "    additional_text_fields=[\"jd_expertise\", \"jd_requirements\"],\n",
    ")\n",
    "```\n",
    "\n",
    "Then re-run this notebook - no code changes needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
